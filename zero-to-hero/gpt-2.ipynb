{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyP6st4cFprBJXY0QW2ZGYax"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7D2KsXPijdBS"},"outputs":[],"source":["from dataclasses import dataclass\n","import torch\n","import torch.nn as nn\n","import inspect\n","from torch.nn import functional as F\n","import math"]},{"cell_type":"code","source":["class CausalSelfAttention(nn.Module):\n","\n","  def __init__(self,config):\n","    super().__init__()\n","    assert config.n_embd % config.n_head == 0\n","    self.c_attn=nn.Linear(config.n_embd,3*config.n_embd)\n","    self.c_proj=nn.Linear(config.n_embd,config.n_embd)\n","    self.c_proj.NANOGPT_SCALE_INIT=1\n","    self.n_head=config.n_head\n","    self.n_embd=config.n_embd\n","    # 因果矩阵掩码\n","    self.register_buffer(\"bias\",torch.tril(torch.ones(config.block_size,config.block_size))\n","                  .view(1,1,config.block_size,config.block_size))\n","\n","  def forward(self,x):\n","    B,T,C=x.size()\n","    # 生成Q,K,V\n","    qkv=self.c_attn(x) #（B,T,3*C）\n","    q,k,v=qkv.split(self.n_embd,dim=2) #(B,T,C)\n","    k=k.view(B,T,self.n_head,C//self.n_head).transpose(1,2) #（B,nh,T,hs）\n","    q=q.view(B,T,self.n_head,C//self.n_head).transpose(1,2) #（B,nh,T,hs）\n","    v=v.view(B,T,self.n_head,C//self.n_head).transpose(1,2) #（B,nh,T,hs）\n","\n","    # 计算注意力分数\n","    # att=(q@k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1)))\n","    # 应用因果掩码\n","    # att=att.masked_fill(self.bias[:,:,:T,:T]==0,float('-inf'))\n","    # softmax + 加权求和\n","    # att=F.softmax(att, dim=-1)\n","    # y=att@v\n","\n","    # Flash Attention\n","    y=F.scaled_dot_product_attention(q,k,v,is_causal=True)\n","    # 重塑\n","    y=y.transpose(1,2).contiguous().view(B,T,C)\n","    y=self.c_proj(y)\n","    return y\n","\n","class MLP(nn.Module):\n","\n","  def __init__(self,config):\n","    super().__init__()\n","    self.c_fc=nn.Linear(config.n_embd,4*config.n_embd)\n","    self.gelu=nn.GELU(approximate='tanh')\n","    self.c_proj=nn.Linear(4*config.n_embd,config.n_embd)\n","    self.c_proj.NANOGPT_SCALE_INIT=1\n","\n","  def forward(self,x):\n","    x=self.c_fc(x)\n","    x=self.gelu(x)\n","    x=self.c_proj(x)\n","    return x\n","\n","class Block(nn.Module):\n","\n","  def __init__(self,config):\n","    super().__init__()\n","    self.ln_1=nn.LayerNorm(config.n_embd)\n","    self.attn=CausalSelfAttention(config)\n","    self.ln_2=nn.LayerNorm(config.n_embd)\n","    self.mlp=MLP(config)\n","\n","  def forward(self,x):\n","    x=x+self.attn(self.ln_1(x))\n","    x=x+self.mlp(self.ln_2(x))\n","    return x\n","\n","@dataclass\n","class GPTConfig:\n","  block_size: int = 1024\n","  vocab_size: int = 50257\n","  n_layer: int = 12\n","  n_head: int = 12\n","  n_embd: int = 768\n","\n","class GPT(nn.Module):\n","\n","  def __init__(self,config):\n","    super().__init__()\n","    self.config=config\n","\n","    self.transformer=nn.ModuleDict(dict(\n","        wte=nn.Embedding(config.vocab_size,config.n_embd),\n","        wpe=nn.Embedding(config.block_size,config.n_embd),\n","        h=nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n","        ln_f=nn.LayerNorm(config.n_embd),\n","    ))\n","    self.lm_head=nn.Linear(config.n_embd,config.vocab_size,bias=False)\n","\n","    # weight sharing 模式\n","    self.transformer.wte.weight=self.lm_head.weight\n","\n","    self.apply(self._init_weights)\n","\n","  def _init_weights(self,module):\n","    if isinstance(module,nn.Linear):\n","      std=0.02\n","      if hasattr(module,'NANOGPT_SCALE_INIT'):\n","        std*=(2*self.config.n_layer)**-0.5\n","      torch.nn.init.normal_(module.weight,mean=0.0,std=std)\n","      if module.bias is not None:\n","        torch.nn.init.zeros_(module.bias)\n","    elif isinstance(module,nn.Embedding):\n","      torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n","\n","\n","  def forward(self,idx,targets=None):\n","    B,T=idx.size()\n","    assert T <= self.config.block_size,f\"Cannot forward sequence of length{T},block size is {self.config.block_size}\"\n","    # forward the token and position embeddings\n","    pos=torch.arange(0,T,dtype=torch.long,device=idx.device)\n","    pos_emb=self.transformer.wpe(pos)\n","    tok_emb=self.transformer.wte(idx)\n","    x=tok_emb+pos_emb\n","    # the blocks of the transformer\n","    for block in self.transformer.h:\n","      x=block(x)\n","    x=self.transformer.ln_f(x)\n","    logits=self.lm_head(x) #(B,T,vocab_size)\n","    loss=None\n","    if targets is not None:\n","      loss=F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n","    return logits,loss\n","\n","  @classmethod\n","  def from_pretrained(cls,model_type):\n","    assert model_type in {'gpt2','gpt2-medium','gpt2-large','gpt2-xl'}\n","    from transformers import GPT2LMHeadModel\n","    print(\"Loading weights from pretrained gpt: %s\" % model_type)\n","    # 定义配置字典\n","    config_args={\n","        'gpt2':    dict(n_layer=12,n_head=12,n_embd=768), # 124M params\n","        'gpt2-medium': dict(n_layer=24,n_head=16,n_embd=1024),# 350M params\n","        'gpt2-large': dict(n_layer=36,n_head=20,n_embd=1280),# 774M params\n","        'gpt2-xl':  dict(n_layer=48,n_head=25,n_embd=1600),# 1558M params\n","    }[model_type]\n","    config_args['vocab_size']=50257\n","    config_args['block_size']=1024\n","\n","    config=GPTConfig(**config_args)\n","    model=GPT(config)\n","    sd=model.state_dict()\n","    sd_keys=sd.keys()\n","    # 过滤掉 attention bias\n","    sd_keys=[k for k in sd_keys if not k.endswith('.attn.bias')]\n","    # 初始化 huggingface/transformers model\n","    model_hf=GPT2LMHeadModel.from_pretrained(model_type)\n","    sd_hf=model_hf.state_dict()\n","\n","    # 权重映射和复制\n","    sd_keys_hf=sd_hf.keys()\n","    sd_keys_hf=[k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n","    sd_keys_hf=[k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n","    transposed=['attn.c_attn.weight','attn.c_proj.weight','mlp.c_fc.weight','mlp.c_proj.weight']\n","    assert len(sd_keys)==len(sd_keys_hf),f\"{len(sd_keys)} != {len(sd_keys_hf)}\"\n","    for k in sd_keys_hf:\n","      if any(k.endswith(w) for w in transposed):\n","        assert sd_hf[k].shape[::-1]==sd[k].shape\n","        with torch.no_grad():\n","          sd[k].copy_(sd_hf[k].t())\n","      else:\n","        assert sd_hf[k].shape==sd[k].shape\n","        with torch.no_grad():\n","          sd[k].copy_(sd_hf[k])\n","\n","    return model\n","\n","  def configure_optimizers(self,weight_decay,learning_rate,device):\n","    # 获取模型所有参数\n","    param_dict={pn:p for pn,p in self.named_parameters()}\n","    # 筛选出带有梯度的参数\n","    param_dict={pn:p for pn,p in param_dict.items() if p.requires_grad}\n","    # 所有二维参数都会被衰减，其他不会\n","    decay_params=[p for n,p in param_dict.items() if p.dim()>=2]\n","    nodecay_params=[p for n,p in param_dict.items() if p.dim()<2]\n","    optim_groups=[\n","        {'params':decay_params,'weight_decay':weight_decay},\n","        {'params':nodecay_params,'weight_decay':0.0}\n","    ]\n","    num_decay_params=sum(p.numel() for p in decay_params)\n","    num_nodecay_params=sum(p.numel() for p in nodecay_params)\n","    print(f\"num decayed tensors: {len(decay_params)},with {num_decay_params:,} params\")\n","    print(f\"num non-decayed tensors: {len(nodecay_params)},with {num_nodecay_params:,} params\")\n","\n","    # 使用Fused AdamW版本\n","    fused_available='fused' in inspect.signature(torch.optim.AdamW).parameters\n","    use_fused=fused_available and 'cuda' in device\n","    print(f\"using fused AdamW:{use_fused}\")\n","    optimizer=torch.optim.AdamW(optim_groups,lr=learning_rate,betas=(0.9,0.95),eps=1e-8,fused=use_fused)\n","    return optimizer\n"],"metadata":{"id":"R7g-RoCrPhIs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMZzBU_TCHp7","executionInfo":{"status":"ok","timestamp":1757989634841,"user_tz":-480,"elapsed":208,"user":{"displayName":"kevin Pan","userId":"16426210367159414238"}},"outputId":"6b41c818-82d2-4377-8330-33e105379476"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-09-16 02:27:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n","\n","2025-09-16 02:27:13 (17.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}]},{"cell_type":"code","source":["import tiktoken\n","\n","class DataLoaderLite:\n","  def __init__(self,B,T):\n","    self.B=B\n","    self.T=T\n","\n","    with open('input.txt','r') as f:\n","      text=f.read()\n","    enc=tiktoken.get_encoding('gpt2')\n","    tokens=enc.encode(text)\n","    self.tokens=torch.tensor(tokens)\n","    print(f\"loaded {len(self.tokens)} tokens\")\n","    print(f\"1 epoch = { len(self.tokens) // (B*T)} batches\")\n","\n","    self.current_position=0\n","\n","  def next_batch(self):\n","    B,T=self.B,self.T\n","    buf=self.tokens[self.current_position:self.current_position+B*T+1]\n","    x=buf[:-1].view(B,T)\n","    y=buf[1:].view(B,T)\n","\n","    self.current_position+=B*T\n","    # reset\n","    if self.current_position+(B*T+1) >= len(self.tokens):\n","      self.current_position=0\n","    return x,y\n","\n","device=\"cpu\"\n","if torch.cuda.is_available():\n","  device=\"cuda\"\n","elif torch.backends.mps.is_available():\n","  device=\"mps\"\n","print(f\"Using device: {device}\")\n","\n","torch.manual_seed(1337)\n","if torch.cuda.is_available():\n","  torch.cuda.manual_seed(1337)\n","\n","# gradient accumulation\n","total_batch_size=524288 # 2**19 ~0.5M\n","B=16\n","T=1024\n","assert total_batch_size % (B*T) == 0\n","grad_accum_steps=total_batch_size // (B*T)\n","print(f\"grad_accum_steps = {grad_accum_steps}\")\n","\n","train_loader=DataLoaderLite(B=16,T=1024)\n","\n","#-------------------------------------------------------------------------------\n","torch.set_float32_matmul_precision('high')\n","\n","import time\n","\n","# get logits\n","model=GPT(GPTConfig(vocab_size=50304))\n","model.to(device)\n","model=torch.compile(model)\n","\n","# warmup && cosine decay\n","max_lr=6e-4\n","min_lr=max_lr*0.1\n","warmup_steps=10\n","max_steps=50\n","def get_lr(it):\n","  if it < warmup_steps:\n","    return max_lr*(it+1)/warmup_steps\n","\n","  if it > max_steps:\n","    return min_lr\n","\n","  decay_ratio=(it-warmup_steps)/(max_steps-warmup_steps)\n","  assert 0<=decay_ratio<=1\n","  coeff=0.5*(1.0+math.cos(math.pi*decay_ratio))\n","  return min_lr+(max_lr-min_lr)*coeff\n","\n","# optimize\n","optimizer=model.configure_optimizers(weight_decay=0.1,learning_rate=6e-4,device=device)\n","\n","for step in range(max_steps):\n","  t0=time.time()\n","  optimizer.zero_grad()\n","  loss_accum=0.0\n","  for micro_step in range(grad_accum_steps):\n","    x,y=train_loader.next_batch()\n","    x,y=x.to(device),y.to(device)\n","    with torch.autocast(device_type=device,dtype=torch.bfloat16):\n","      logits,loss=model(x,y)\n","    loss=loss/grad_accum_steps\n","    loss_accum+=loss.detach()\n","    loss.backward()\n","  norm=torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n","  # determine and set the learning rate for this iteration\n","  lr=get_lr(step)\n","  for param_group in optimizer.param_groups:\n","    param_group['lr']=lr\n","  optimizer.step()\n","  # wait for GPU to finish work\n","  torch.cuda.synchronize()\n","  t1=time.time()\n","  dt=t1-t0\n","  tokens_processed=train_loader.B * train_loader.T * grad_accum_steps\n","  tokens_per_sec=tokens_processed / dt\n","  print(f\"step {step:4d}|loss:{loss_accum.item():.6f}|lr:{lr:.4e}|norm:{norm:.4f}|dt:{dt*1000:.2f}ms|tok/sec:{tokens_per_sec:.2f}\")\n","\n","\n","import sys;sys.exit(0)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lLlt-jh_TFFr","executionInfo":{"status":"error","timestamp":1757989812922,"user_tz":-480,"elapsed":176831,"user":{"displayName":"kevin Pan","userId":"16426210367159414238"}},"outputId":"65d5c048-af72-47f3-e873-6acf22ab27dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","grad_accum_steps = 32\n","loaded 338025 tokens\n","1 epoch = 20 batches\n","num decayed tensors: 50,with 124,354,560 params\n","num non-decayed tensors: 98,with 121,344 params\n","using fused AdamW:True\n","step    0|loss:10.938565|lr:6.0000e-05|norm:27.0125|dt:27615.35ms|tok/sec:18985.39\n","step    1|loss:9.649336|lr:1.2000e-04|norm:9.5176|dt:2772.30ms|tok/sec:189116.67\n","step    2|loss:9.225581|lr:1.8000e-04|norm:5.7287|dt:2778.90ms|tok/sec:188667.21\n","step    3|loss:9.813110|lr:2.4000e-04|norm:8.2066|dt:2775.81ms|tok/sec:188877.45\n","step    4|loss:9.191634|lr:3.0000e-04|norm:4.2995|dt:2773.14ms|tok/sec:189059.26\n","step    5|loss:8.678026|lr:3.6000e-04|norm:3.6285|dt:2778.27ms|tok/sec:188710.13\n","step    6|loss:8.294989|lr:4.2000e-04|norm:1.9536|dt:2780.59ms|tok/sec:188552.87\n","step    7|loss:8.068013|lr:4.8000e-04|norm:2.8517|dt:2778.75ms|tok/sec:188677.60\n","step    8|loss:7.714189|lr:5.4000e-04|norm:1.9113|dt:2776.39ms|tok/sec:188837.97\n","step    9|loss:7.347027|lr:6.0000e-04|norm:1.8001|dt:2782.65ms|tok/sec:188412.93\n","step   10|loss:7.029688|lr:6.0000e-04|norm:1.8388|dt:2784.53ms|tok/sec:188286.06\n","step   11|loss:6.741205|lr:5.9917e-04|norm:1.5058|dt:2783.01ms|tok/sec:188388.65\n","step   12|loss:6.528881|lr:5.9668e-04|norm:1.1515|dt:2784.16ms|tok/sec:188310.99\n","step   13|loss:6.377279|lr:5.9254e-04|norm:1.0577|dt:2788.37ms|tok/sec:188026.98\n","step   14|loss:6.341538|lr:5.8679e-04|norm:2.6008|dt:2786.32ms|tok/sec:188165.13\n","step   15|loss:6.243542|lr:5.7945e-04|norm:0.9747|dt:2787.38ms|tok/sec:188093.53\n","step   16|loss:6.212478|lr:5.7057e-04|norm:0.7723|dt:2793.22ms|tok/sec:187700.41\n","step   17|loss:6.210046|lr:5.6021e-04|norm:1.1331|dt:2795.04ms|tok/sec:187577.91\n","step   18|loss:6.156593|lr:5.4843e-04|norm:1.0137|dt:2796.14ms|tok/sec:187503.87\n","step   19|loss:6.146916|lr:5.3531e-04|norm:1.4957|dt:2790.61ms|tok/sec:187875.83\n","step   20|loss:6.121985|lr:5.2092e-04|norm:1.0395|dt:2792.95ms|tok/sec:187718.54\n","step   21|loss:6.066035|lr:5.0535e-04|norm:0.7654|dt:2798.86ms|tok/sec:187322.07\n","step   22|loss:6.053053|lr:4.8870e-04|norm:0.5361|dt:2797.52ms|tok/sec:187411.87\n","step   23|loss:6.005051|lr:4.7107e-04|norm:0.4912|dt:2799.65ms|tok/sec:187269.11\n","step   24|loss:5.992750|lr:4.5258e-04|norm:0.4031|dt:2799.34ms|tok/sec:187289.86\n","step   25|loss:5.979293|lr:4.3332e-04|norm:0.3671|dt:2798.17ms|tok/sec:187368.15\n","step   26|loss:5.957904|lr:4.1343e-04|norm:0.4127|dt:2798.71ms|tok/sec:187332.03\n","step   27|loss:5.971112|lr:3.9303e-04|norm:0.4904|dt:2800.39ms|tok/sec:187219.75\n","step   28|loss:5.940405|lr:3.7224e-04|norm:0.3396|dt:2803.47ms|tok/sec:187014.02\n","step   29|loss:5.933563|lr:3.5118e-04|norm:0.3109|dt:2800.72ms|tok/sec:187197.24\n","step   30|loss:5.927957|lr:3.3000e-04|norm:0.3056|dt:2800.77ms|tok/sec:187194.17\n","step   31|loss:5.903971|lr:3.0882e-04|norm:0.3248|dt:2802.68ms|tok/sec:187066.79\n","step   32|loss:5.909022|lr:2.8776e-04|norm:0.3366|dt:2802.38ms|tok/sec:187086.76\n","step   33|loss:5.884777|lr:2.6697e-04|norm:0.5438|dt:2801.46ms|tok/sec:187148.16\n","step   34|loss:5.873114|lr:2.4657e-04|norm:0.3027|dt:2808.26ms|tok/sec:186694.67\n","step   35|loss:5.874592|lr:2.2668e-04|norm:0.6185|dt:2806.54ms|tok/sec:186809.55\n","step   36|loss:5.851648|lr:2.0742e-04|norm:0.2510|dt:2805.84ms|tok/sec:186855.86\n","step   37|loss:5.869135|lr:1.8893e-04|norm:0.3818|dt:2803.28ms|tok/sec:187026.78\n","step   38|loss:5.840863|lr:1.7130e-04|norm:0.3002|dt:2808.64ms|tok/sec:186669.96\n","step   39|loss:5.837390|lr:1.5465e-04|norm:0.1895|dt:2806.78ms|tok/sec:186793.38\n","step   40|loss:5.840047|lr:1.3908e-04|norm:0.2781|dt:2802.17ms|tok/sec:187100.52\n","step   41|loss:5.821765|lr:1.2469e-04|norm:0.2081|dt:2805.83ms|tok/sec:186856.45\n","step   42|loss:5.840686|lr:1.1157e-04|norm:0.2753|dt:2808.86ms|tok/sec:186655.03\n","step   43|loss:5.814506|lr:9.9787e-05|norm:0.2884|dt:2807.40ms|tok/sec:186751.89\n","step   44|loss:5.811653|lr:8.9428e-05|norm:0.1872|dt:2804.33ms|tok/sec:186956.46\n","step   45|loss:5.813104|lr:8.0553e-05|norm:0.2008|dt:2809.52ms|tok/sec:186611.33\n","step   46|loss:5.798387|lr:7.3215e-05|norm:0.2559|dt:2808.70ms|tok/sec:186665.47\n","step   47|loss:5.818125|lr:6.7460e-05|norm:0.1801|dt:2807.75ms|tok/sec:186728.54\n","step   48|loss:5.791354|lr:6.3324e-05|norm:0.1710|dt:2806.45ms|tok/sec:186815.41\n","step   49|loss:5.791154|lr:6.0832e-05|norm:0.1974|dt:2810.11ms|tok/sec:186572.22\n"]},{"output_type":"error","ename":"SystemExit","evalue":"0","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qbaDVCm2Y1J9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LTkqrn5yM1ko"},"execution_count":null,"outputs":[]}]}